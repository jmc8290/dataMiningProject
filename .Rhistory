dist_to_medoids <- dmat[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
all1 <- predict_pam(full_data_scaled, pam1, s1)
sq
s1
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, setdiff(names(df_train), "cluster"), drop = FALSE]
newdata  <- newdata[, setdiff(names(newdata), "cluster"), drop = FALSE]
df_train <- df_train[, setdiff(names(df_train), "cluste_relabr"), drop = FALSE]
newdata  <- newdata[, setdiff(names(newdata), "cluster_relab"), drop = FALSE]
# Extract medoid rows from training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Compute Gower distance between newdata and the medoids
dmat <- daisy(rbind(medoids, newdata), metric = "gower") |> as.matrix()
# Distance block for newdata rows â†’ medoids
k <- nrow(medoids)
dist_to_medoids <- dmat[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
all1 <- predict_pam(full_data_scaled, pam1, s1)
predict_pam <- function(newdata, pam_obj, df_train) {
# Remove possible cluster column
df_train <- df_train[, setdiff(names(df_train), "cluster"), drop = FALSE]
newdata  <- newdata[, setdiff(names(newdata), "cluster"), drop = FALSE]
# --- Align columns exactly ---
newdata <- newdata[, names(df_train), drop = FALSE]
# --- Harmonize factor levels ---
for (col in names(df_train)) {
if (is.factor(df_train[[col]])) {
new_levels <- levels(df_train[[col]])
newdata[[col]] <- factor(newdata[[col]], levels = new_levels)
}
}
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
ncol(split1)
ncol(s1)
s1
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, -((ncol(df_train)-1):ncol(df_train))]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
assignments_split2 <- predict_pam(split2, pam2, s2)
assignments_split3 <- predict_pam(split3, pam3, s3)
relabel <- function(cluster_vec, map) {
map[cluster_vec]
}
assignments_split2_aligned <- relabel(assignments_split2, map12)
assignments_split3_aligned <- relabel(assignments_split3, map13)
split1$cluster_model1 <- assignments_split1
split2$cluster_model2 <- assignments_split2_aligned
split3$cluster_model3 <- assignments_split3_aligned
full_data_with_clusters <- rbind(split1, split2, split3)
ncol(split1)
ncol(split2)
ncol(split3)
split1
split2
colnames(split1)[colnames(split1) == "cluster_model1"] <- "clusterLabel"
colnames(split2)[colnames(split2) == "cluster_model2"] <- "clusterLabel"
colnames(split3)[colnames(split3) == "cluster_model3"] <- "clusterLabel"
full_data_with_clusters <- rbind(split1, split2, split3)
unique(full_data_with_clusters$clusterLabel)
cluster1_data <- subset(full_data_scaled, clusterLabel == 1)
cluster1_data <- subset(full_data_with_clusters, clusterLabel == 1)
cluster1_data <- subset(full_data_with_clusters, clusterLabel == 1)
cluster2_data <- subset(full_data_with_clusters, clusterLabel == 2)
cluster3_data <- subset(full_data_with_clusters, clusterLabel == 3)
cluster4_data <- subset(full_data_with_clusters, clusterLabel == 4)
cluster5_data <- subset(full_data_with_clusters, clusterLabel == 5)
cluster1_data
cluster1_data #Married, White males potentially
c(mean(cluster1_data$incomeGreaterThan50k), mean(cluster1_data$race.White), mean(cluster1_data$marital.status.Married.civ.spouse), mean(cluster1_data$sex.Male))
cluster2_data
cluster2_data #neverMarried, poor, white, males
c(mean(cluster2_data$marital.status.Never.married), 1 - mean(cluster2_data$incomeGreaterThan50k), mean(cluster2_data$race.White), mean(cluster2_data$sex.Male))
c(mean(full_data_with_clusters$marital.status.Married.civ.spouse), mean(full_data_with_clusters$race.White), mean(full_data_with_clusters$incomeGreaterThan50k), mean(full_data_with_clusters$sex.Male))
cluster3_data
cluster3_data #college dropouts/inschool, never married, women
c(mean(cluster3_data$incomeGreaterThan50k), mean(cluster3_data$sex.Female), mean(cluster3_data$education.Some.college), mean(cluster3_data$marital.status.Never.married), mean(cluster3_data$race.White))
cluster3_data #college dropouts/inschool, never married, women
c(1 - mean(cluster3_data$incomeGreaterThan50k), mean(cluster3_data$sex.Female), mean(cluster3_data$education.Some.college), mean(cluster3_data$marital.status.Never.married), mean(cluster3_data$race.White))
mean(full_data_with_clusters$education.Some.college)
cluster4_data
cluster4_data #private sector, divorced, women
c(mean(cluster4_data$workclass.Private), mean(cluster4_data$marital.status.Divorced), 1 - mean(cluster4_data$incomeGreaterThan50k), mean(cluster4_data$sex.Female), mean(cluster4_data$race.White))
mean(full_data_scaled$workclass.Private)
mean(full_data_with_clusters$marital.status.Divorced)
cluster5_data
cluster5_data #highest education highschool, married, male,
c(mean(cluster5_data$race.White), mean(cluster5_data$sex.Male), mean(cluster5_data$marital.status.Married.civ.spouse), 1 - mean(cluster5_data$incomeGreaterThan50k)
c(mean(cluster5_data$race.White), mean(cluster5_data$sex.Male), mean(cluster5_data$marital.status.Married.civ.spouse), 1 - mean(cluster5_data$incomeGreaterThan50k))
mean(full_data_with_clusters$education.HS.grad)
library(caret)
library(randomForest)
# Setting seed to ensure results are reproducible
set.seed(123)
library(caret)
library(randomForest)
# Setting seed to ensure results are reproducible
set.seed(123)
# Matrix/data frame of predictors (all numeric after encoding)
x_all <- predict(dummy_model, newdata = full_data_scaled)
library(caret)
library(randomForest)
# Setting seed to ensure results are reproducible
set.seed(123)
full_data_scaled[, ncol(full_data_scaled)] <-
as.factor(full_data_scaled[, ncol(full_data_scaled)])
train_index <- createDataPartition(
y = full_data_scaled[, ncol(full_data_scaled)],
p = 0.8,
list = FALSE
)
train_data <- full_data_scaled[train_index, ]
test_data  <- full_data_scaled[-train_index, ]
rf_model <- randomForest(
x = train_data[, -ncol(train_data)],   # predictors
y = train_data[,  ncol(train_data)],
ntree = 500,                            # number of trees in the forest
mtry = floor(sqrt(ncol(x) - 1)),     # number of predictors sampled per split
importance = TRUE                     # compute variable importance
)
rf_model <- randomForest(
x = train_data[, -ncol(train_data)],   # predictors
y = train_data[,  ncol(train_data)],
ntree = 500,                            # number of trees in the forest
mtry = floor(sqrt(ncol(train_data) - 1)),     # number of predictors sampled per split
importance = TRUE                     # compute variable importance
)
rf_model
rf_pred_class <- predict(rf_model, newdata = x_test, type = "class")
rf_pred_class <- predict(rf_model, newdata = test_data, type = "class")
confusionMatrix(
data = rf_pred_class,
reference = test_data[, ncol(test_data)]
)
rf_probs <- predict(
rf_model,
test_data[, -ncol(test_data)],
type = "prob"
)[, 2]
roc_obj <- roc(
response = test_data[, ncol(test_data)],
predictor = rf_probs
)
library(pROC)
library(pROC)
rf_probs <- predict(
rf_model,
test_data[, -ncol(test_data)],
type = "prob"
)[, 2]
roc_obj <- roc(
response = test_data[, ncol(test_data)],
predictor = rf_probs
)
auc(roc_obj)
plot(roc_obj)
importance(rf_model)
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 1]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$Importance), ]
# Get top 5 most important variables
top5_vars <- head(var_imp_df, 5)
top5_vars
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 1]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$MeanDecreaseAccuracy), ]
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 2]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$Importance, ]
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 2]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$Importance), ]
# Get top 5 most important variables
top5_vars <- head(var_imp_df, 5)
top5_vars
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 1]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$Importance), ]
# Get top 5 most important variables
top5_vars <- head(var_imp_df, 5)
top5_vars
varImpPlot(rf_model)
varImpPlot(rf_model, n.var = 5)
varImpPlot(rf_model, n.var = 10)
varImpPlot(rf_model, n.var = 10, type = 1)
knitr::opts_chunk$set(echo = TRUE)
full_data <- read.csv('data/data_mining.csv')
head(full_data)
# 1. Remove unnecessary ID/index column
full_data$X <- NULL
# 2. Replace '?' with NA (for missing values)
full_data[full_data == "?"] <- NA
# 3. Check how many missing values per column
colSums(is.na(full_data))
# 4. Remove rows with missing values
full_data <- na.omit(full_data)
colSums(is.na(full_data))
# 4.1 Clean income variable column
full_data$income <- gsub("\\.", "", full_data$income)
# 5. Scale numeric columns (regularization)
library(dplyr)
full_data_scaled <- full_data %>%
mutate(across(where(is.numeric), scale))
# 6. Convert character columns to factors
full_data_scaled[sapply(full_data_scaled, is.character)] <-
lapply(full_data_scaled[sapply(full_data_scaled, is.character)], as.factor)
# 7. One-hot encode categorical variables (dummy variables)
library(caret)
dummy <- dummyVars(" ~ .", data = full_data_scaled)
full_data_scaled <- data.frame(predict(dummy, newdata = full_data_scaled))
#7.2 Drop unnecessary income column
full_data_scaled <- full_data_scaled[, - (ncol(full_data_scaled) - 1)]
#7.3 Rename the income column
colnames(full_data_scaled)[ncol(full_data_scaled)] <- "incomeGreaterThan50k"
# 8. Check structure of the processed dataset
str(full_data_scaled)
# 9. Preview cleaned and processed data
head(full_data_scaled)
ruleData <- subset(full_data, select = c(education, marital.status, occupation, race, sex, native.country))
ruleData <- as.data.frame(model.matrix(~ . -1, data = ruleData))
head(ruleData)
library(arules)
data_transactions <- as(as.data.frame(lapply(ruleData, function(x) as.logical(x))), "transactions")
itemsets <- apriori(data_transactions, parameter = list(target = 'frequent itemsets', support = 0.01))
rules <- apriori(data_transactions, parameter = list(support = 0.01, confidence = 0.5))
inspect(itemsets)
rule_data <- read.csv('data/data_mining.csv')
ruleData <- subset(rule_data, select = c(education, marital.status, occupation, race, sex, native.country))
ruleData <- as.data.frame(model.matrix(~ . -1, data = ruleData))
head(ruleData)
library(arules)
data_transactions <- as(as.data.frame(lapply(ruleData, function(x) as.logical(x))), "transactions")
itemsets <- apriori(data_transactions, parameter = list(target = 'frequent itemsets', support = 0.01))
rules <- apriori(data_transactions, parameter = list(support = 0.01, confidence = 0.5))
inspect(itemsets)
inspect(rules)
rules <- rules[!is.redundant(rules)]
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = 'lift')[11:20])
inspect(sort(rules, by = 'lift')[21:30])
inspect(sort(rules, by = 'lift')[31:40])
inspect(sort(rules, by = 'lift')[41:50])
inspect(sort(rules, by = 'lift')[51:63])
marriageCounts <- table(rule_data$marital.status)
barplot(counts,
main = 'Marital Status Proportions',
xlab = 'Marital Status',
ylab = 'Proportions',
col = '#0047BA')
marriageCounts <- table(rule_data$marital.status)
barplot(marriageCounts,
main = 'Marital Status Proportions',
xlab = 'Marital Status',
ylab = 'Proportions',
col = '#0047BA')
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(fill = "0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group1), fill = "0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
marriage <- rule_data$marital.status
levels(marriage)
marriage <- rule_data$marital.status
marraige
marriage
levels(marriage)
levels(rule_data$marital.status)
rule_data <- rule_data %>%
mutate(marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
library(dplyr)
rule_data <- rule_data %>%
mutate(marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = dplyr::recode(marital.status,
"Married-civ-spouse" = "Married"))
library(ggplot2)
rule_data$marital.status
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = dplyr::recode(marital.status,
"Married-civ-spouse" = "Married",
"Married-spouse-absent" = "Spouse Absent"))
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
ggsave("barplotM.png")
barplotM <- ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
ggsave("barplotM.png")
library(ggplot2)
barplotM <- ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
#ggsave("barplotM.png")
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
#ggsave("barplotM.png")
head(rule_data)
ggplot(rule_data, aes(x = race)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Race Proportions",
x = "Race",
y = "Proportion")
prop.table(rule_data$race)
prop.table(table(rule_data$race))
ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
rule_data$income <- gsub("\\.", "", rule_data$income)
ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
rule_data$income <- gsub("\\.", "", rule_data$income)
income <- ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
ggsave('income.png')
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education",
ylab = "Density",
col = "#0047BA",
lwd = 2)
max(rule_data$education.num)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xticks(seq(0, 16), by = 2),
col = "#0047BA",
lwd = 2)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
png("education_density.png")
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
dev.off()
mean(full_data_scaled$incomeGreaterThan50k)
mean(full_data_scaled$race.White)
cluster2_data$education.num
length(cluster2_data)
length(cluster1_data)
nrow(cluster2_data)
nrow(cluster1_data)
nrow(cluster3_data)
nrow(cluster4_data)
nrow(cluster5_data)
mean(cluster2_data$education.HS.grad)
mean(full_data_with_clusters$education.HS.grad)
mean(cluster2_data$education.1st.4th) + mean(cluster2_data$education.5th.6th) + mean(cluster2_data$education.7th.8th) + mean(cluster2_data$education.9th) + mean(cluster2_data$education.10th) + mean(cluster2_data$education.11th) + mean(cluster2_data$education.12th) + mean(cluster2_data$education.HS.grad)
mean(full_data_scaled$education.1st.4th) +
mean(full_data_scaled$education.5th.6th) +
mean(full_data_scaled$education.7th.8th) +
mean(full_data_scaled$education.9th) +
mean(full_data_scaled$education.10th) +
mean(full_data_scaled$education.11th) +
mean(full_data_scaled$education.12th) +
mean(full_data_scaled$education.HS.grad)
mean(full_data_scaled$education.Some.college)
mean(cluster4_data$incomeGreaterThan50k)
1 - mean(cluster4_data$incomeGreaterThan50k)
cluster4_data # divorced, women
#Average divorce rate in the data set is 0.138 compared to 0.72 here
c(mean(cluster4_data$marital.status.Divorced), 1 - mean(cluster4_data$incomeGreaterThan50k), mean(cluster4_data$sex.Female), mean(cluster4_data$race.White))
c(mean(cluster5_data$race.White), mean(cluster5_data$sex.Male), mean(cluster5_data$marital.status.Married.civ.spouse))
mean(cluster5_data$education.HS.grad)
mean(cluster5_data$education.1st.4th) +
mean(cluster5_data$education.5th.6th) +
mean(cluster5_data$education.7th.8th) +
mean(cluster5_data$education.9th) +
mean(cluster5_data$education.10th) +
mean(cluster5_data$education.11th) +
mean(cluster5_data$education.12th) +
mean(cluster5_data$education.HS.grad)
mean(cluster5_data$incomeGreaterThan50k)
mean(cluster5_data$age)
mean(cluster2_data$age)
muAge <- mean(rule_data$age)
sdA <- sd(rule_data$age)
muAge <- mean(rule_data$age)
sdA <- sd(rule_data$age)
mu1 <- mean(cluster1_data$age)*sdA + muAge
mu2 <- mean(cluster2_data$age)*sdA + muAge
mu3 <- mean(cluster3_data$age)*sdA + muAge
mu4 <- mean(cluster4_data$age)*sdA + muAge
mu5 <- mean(cluster5_data$age)*sdA + muAge
c(mu1, mu2, mu3, mu4, mu5)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
mean(cluster4_data$education.Some.college)
mean(full_data_with_clusters$education.Some.college)
mean(cluster4_data$education.Prof.school) + mean(cluster4_data$education.Assoc.acdm) + mean(cluster4_data$education.Assoc.voc) + mean(cluster4_data$education.Bachelors) + mean(cluster4_data$education.Masters) + mean(cluster4_data$education.Doctorate)
mean(cluster4_data$education.HS.grad)
mean(cluster4_data$workclass.Local.gov) + mean(cluster4_data$workclass.Federal.gov) + mean(cluster4_data$workclass.State.gov)
mean(full_data_scaled$workclass.Local.gov) + mean(full_data_scaled$workclass.State.gov) + mean(full_data_scaled$workclass.Federal.gov)
14/18.2
4.2/14
