levels(rule_data$marital.status)
rule_data <- rule_data %>%
mutate(marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
library(dplyr)
rule_data <- rule_data %>%
mutate(marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = recode(marital.status,
"Married-civ-spouse" = "Married"))
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = dplyr::recode(marital.status,
"Married-civ-spouse" = "Married"))
library(ggplot2)
rule_data$marital.status
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = dplyr::recode(marital.status,
"Married-civ-spouse" = "Married",
"Married-spouse-absent" = "Spouse Absent"))
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
ggsave("barplotM.png")
barplotM <- ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
ggsave("barplotM.png")
library(ggplot2)
barplotM <- ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
#ggsave("barplotM.png")
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
#ggsave("barplotM.png")
head(rule_data)
ggplot(rule_data, aes(x = race)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Race Proportions",
x = "Race",
y = "Proportion")
prop.table(rule_data$race)
prop.table(table(rule_data$race))
ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
rule_data$income <- gsub("\\.", "", rule_data$income)
ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
rule_data$income <- gsub("\\.", "", rule_data$income)
income <- ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
ggsave('income.png')
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education",
ylab = "Density",
col = "#0047BA",
lwd = 2)
max(rule_data$education.num)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xticks(seq(0, 16), by = 2),
col = "#0047BA",
lwd = 2)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
png("education_density.png")
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
dev.off()
mean(full_data_scaled$incomeGreaterThan50k)
mean(full_data_scaled$race.White)
cluster2_data$education.num
length(cluster2_data)
length(cluster1_data)
nrow(cluster2_data)
nrow(cluster1_data)
nrow(cluster3_data)
nrow(cluster4_data)
nrow(cluster5_data)
mean(cluster2_data$education.HS.grad)
mean(full_data_with_clusters$education.HS.grad)
mean(cluster2_data$education.1st.4th) + mean(cluster2_data$education.5th.6th) + mean(cluster2_data$education.7th.8th) + mean(cluster2_data$education.9th) + mean(cluster2_data$education.10th) + mean(cluster2_data$education.11th) + mean(cluster2_data$education.12th) + mean(cluster2_data$education.HS.grad)
mean(full_data_scaled$education.1st.4th) +
mean(full_data_scaled$education.5th.6th) +
mean(full_data_scaled$education.7th.8th) +
mean(full_data_scaled$education.9th) +
mean(full_data_scaled$education.10th) +
mean(full_data_scaled$education.11th) +
mean(full_data_scaled$education.12th) +
mean(full_data_scaled$education.HS.grad)
mean(full_data_scaled$education.Some.college)
mean(cluster4_data$incomeGreaterThan50k)
1 - mean(cluster4_data$incomeGreaterThan50k)
cluster4_data # divorced, women
#Average divorce rate in the data set is 0.138 compared to 0.72 here
c(mean(cluster4_data$marital.status.Divorced), 1 - mean(cluster4_data$incomeGreaterThan50k), mean(cluster4_data$sex.Female), mean(cluster4_data$race.White))
c(mean(cluster5_data$race.White), mean(cluster5_data$sex.Male), mean(cluster5_data$marital.status.Married.civ.spouse))
mean(cluster5_data$education.HS.grad)
mean(cluster5_data$education.1st.4th) +
mean(cluster5_data$education.5th.6th) +
mean(cluster5_data$education.7th.8th) +
mean(cluster5_data$education.9th) +
mean(cluster5_data$education.10th) +
mean(cluster5_data$education.11th) +
mean(cluster5_data$education.12th) +
mean(cluster5_data$education.HS.grad)
mean(cluster5_data$incomeGreaterThan50k)
mean(cluster5_data$age)
mean(cluster2_data$age)
muAge <- mean(rule_data$age)
sdA <- sd(rule_data$age)
muAge <- mean(rule_data$age)
sdA <- sd(rule_data$age)
mu1 <- mean(cluster1_data$age)*sdA + muAge
mu2 <- mean(cluster2_data$age)*sdA + muAge
mu3 <- mean(cluster3_data$age)*sdA + muAge
mu4 <- mean(cluster4_data$age)*sdA + muAge
mu5 <- mean(cluster5_data$age)*sdA + muAge
c(mu1, mu2, mu3, mu4, mu5)
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
mean(cluster4_data$education.Some.college)
mean(full_data_with_clusters$education.Some.college)
mean(cluster4_data$education.Prof.school) + mean(cluster4_data$education.Assoc.acdm) + mean(cluster4_data$education.Assoc.voc) + mean(cluster4_data$education.Bachelors) + mean(cluster4_data$education.Masters) + mean(cluster4_data$education.Doctorate)
mean(cluster4_data$education.HS.grad)
mean(cluster4_data$workclass.Local.gov) + mean(cluster4_data$workclass.Federal.gov) + mean(cluster4_data$workclass.State.gov)
mean(full_data_scaled$workclass.Local.gov) + mean(full_data_scaled$workclass.State.gov) + mean(full_data_scaled$workclass.Federal.gov)
14/18.2
4.2/14
knitr::opts_chunk$set(echo = TRUE)
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, -((ncol(df_train)-1):ncol(df_train))]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
df_train1 <- df_train[, -((ncol(df_train)-1):ncol(df_train))]
s6 <- s1[, -((ncol(s1)-1):ncol(s1))]
# Extract medoids from original training data
medoids <- s6[pam1$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
ncol(medoids)
ncol(split1)
split1
set.seed(123)
n <- nrow(full_data_scaled)
split_id <- sample(rep(1:3, length.out = n))
split1 <- full_data_scaled[split_id == 1, ]
split2 <- full_data_scaled[split_id == 2, ]
split3 <- full_data_scaled[split_id == 3, ]
subsample_size <- 2000
s1 <- split1[sample(nrow(split1), subsample_size), ]
s2 <- split2[sample(nrow(split2), subsample_size), ]
s3 <- split3[sample(nrow(split3), subsample_size), ]
pam_run <- function(df, k) {
gower_dist <- daisy(df, metric = "gower")
pam(gower_dist, k = k, diss = TRUE)
}
k <- 5   # choose once and keep fixed
pam1 <- pam_run(s1, k)
knitr::opts_chunk$set(echo = TRUE)
full_data <- read.csv('data/data_mining.csv')
head(full_data)
# 1. Remove unnecessary ID/index column
full_data$X <- NULL
# 2. Replace '?' with NA (for missing values)
full_data[full_data == "?"] <- NA
# 3. Check how many missing values per column
colSums(is.na(full_data))
# 4. Remove rows with missing values
full_data <- na.omit(full_data)
colSums(is.na(full_data))
# 4.1 Clean income variable column
full_data$income <- gsub("\\.", "", full_data$income)
# 5. Scale numeric columns (regularization)
library(dplyr)
full_data_scaled <- full_data %>%
mutate(across(where(is.numeric), scale))
# 6. Convert character columns to factors
full_data_scaled[sapply(full_data_scaled, is.character)] <-
lapply(full_data_scaled[sapply(full_data_scaled, is.character)], as.factor)
# 7. One-hot encode categorical variables (dummy variables)
library(caret)
dummy <- dummyVars(" ~ .", data = full_data_scaled)
full_data_scaled <- data.frame(predict(dummy, newdata = full_data_scaled))
#7.2 Drop unnecessary income column
full_data_scaled <- full_data_scaled[, - (ncol(full_data_scaled) - 1)]
#7.3 Rename the income column
colnames(full_data_scaled)[ncol(full_data_scaled)] <- "incomeGreaterThan50k"
# 8. Check structure of the processed dataset
str(full_data_scaled)
# 9. Preview cleaned and processed data
head(full_data_scaled)
rule_data <- read.csv('data/data_mining.csv')
ruleData <- subset(rule_data, select = c(education, marital.status, occupation, race, sex, native.country))
ruleData <- as.data.frame(model.matrix(~ . -1, data = ruleData))
head(ruleData)
library(arules)
data_transactions <- as(as.data.frame(lapply(ruleData, function(x) as.logical(x))), "transactions")
itemsets <- apriori(data_transactions, parameter = list(target = 'frequent itemsets', support = 0.01))
rules <- apriori(data_transactions, parameter = list(support = 0.01, confidence = 0.5))
inspect(itemsets)
inspect(rules)
rules <- rules[!is.redundant(rules)]
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = 'lift')[11:20])
inspect(sort(rules, by = 'lift')[21:30])
inspect(sort(rules, by = 'lift')[31:40])
inspect(sort(rules, by = 'lift')[41:50])
inspect(sort(rules, by = 'lift')[51:63])
inspect(sort(rules, by = 'lift')[64:75])
library(caret)
library(randomForest)
# Setting seed to ensure results are reproducible
set.seed(123)
full_data_scaled[, ncol(full_data_scaled)] <-
as.factor(full_data_scaled[, ncol(full_data_scaled)])
train_index <- createDataPartition(
y = full_data_scaled[, ncol(full_data_scaled)],
p = 0.8,
list = FALSE
)
train_data <- full_data_scaled[train_index, ]
test_data  <- full_data_scaled[-train_index, ]
rf_model <- randomForest(
x = train_data[, -ncol(train_data)],   # predictors
y = train_data[,  ncol(train_data)],
ntree = 500,                            # number of trees in the forest
mtry = floor(sqrt(ncol(train_data) - 1)),     # number of predictors sampled per split
importance = TRUE                     # compute variable importance
)
rf_model
rf_pred_class <- predict(rf_model, newdata = test_data, type = "class")
confusionMatrix(
data = rf_pred_class,
reference = test_data[, ncol(test_data)]
)
library(pROC)
rf_probs <- predict(
rf_model,
test_data[, -ncol(test_data)],
type = "prob"
)[, 2]
roc_obj <- roc(
response = test_data[, ncol(test_data)],
predictor = rf_probs
)
auc(roc_obj)
plot(roc_obj)
var_imp <- importance(rf_model)
# Convert to a data frame (optional, makes sorting easier)
var_imp_df <- data.frame(
Variable = rownames(var_imp),
Importance = var_imp[, 1]  # for MeanDecreaseGini (default)
)
# Sort in decreasing order
var_imp_df <- var_imp_df[order(-var_imp_df$Importance), ]
# Get top 5 most important variables
top5_vars <- head(var_imp_df, 5)
top5_vars
varImpPlot(rf_model, n.var = 10, type = 1)
library(cluster)
library(factoextra)
library(dplyr)
sampsize <- 2000
samp_idx <- sample(nrow(full_data_scaled), sampsize)
adult_sample <- full_data_scaled[samp_idx, ]
# 2. Gower on SAMPLE ONLY (this is safe)
gower_sample <- daisy(adult_sample, metric = "gower")
# 3. Find best k using PAM + silhouette
k_values <- 2:8
sil_width <- numeric(length(k_values))
for (i in seq_along(k_values)) {
pam_fit <- pam(gower_sample, k = k_values[i], diss = TRUE)
sil_width[i] <- mean(silhouette(pam_fit)[, 3])
}
plot(k_values, sil_width, type = "b")
best_k <- 5
set.seed(123)
n <- nrow(full_data_scaled)
split_id <- sample(rep(1:3, length.out = n))
split1 <- full_data_scaled[split_id == 1, ]
split2 <- full_data_scaled[split_id == 2, ]
split3 <- full_data_scaled[split_id == 3, ]
subsample_size <- 2000
s1 <- split1[sample(nrow(split1), subsample_size), ]
s2 <- split2[sample(nrow(split2), subsample_size), ]
s3 <- split3[sample(nrow(split3), subsample_size), ]
pam_run <- function(df, k) {
gower_dist <- daisy(df, metric = "gower")
pam(gower_dist, k = k, diss = TRUE)
}
k <- 5   # choose once and keep fixed
pam1 <- pam_run(s1, k)
pam2 <- pam_run(s2, k)
pam3 <- pam_run(s3, k)
s1$cluster <- pam1$clustering
s2$cluster <- pam2$clustering
s3$cluster <- pam3$clustering
library(clue)
get_medoid_data <- function(pam_obj, df) {
df[pam_obj$medoids, , drop = FALSE]
}
med1 <- get_medoid_data(pam1, s1)
med2 <- get_medoid_data(pam2, s2)
med3 <- get_medoid_data(pam3, s3)
gower_dist <- function(a, b) {
daisy(rbind(a, b), metric = "gower") |>
as.matrix()|> (\(m) m[1:nrow(a), (nrow(a) + 1):(nrow(a) + nrow(b))])()
}
align_clusters <- function(pamA, dfA, pamB, dfB) {
medA <- get_medoid_data(pamA, dfA)
medB <- get_medoid_data(pamB, dfB)
dist_mat <- gower_dist(medA, medB)
solve_LSAP(dist_mat)
}
map12 <- align_clusters(pam1, s1, pam2, s2)
map13 <- align_clusters(pam1, s1, pam3, s3)
map23 <- align_clusters(pam2, s2, pam3, s3)
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, -((ncol(df_train)-1):ncol(df_train))]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
ncol(split1)
predict_pam <- function(newdata, pam_obj, df_train) {
#df_train <- df_train[, -((ncol(df_train)-1):ncol(df_train))]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
ncol(s1)
s1
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, -ncol(df_train)]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
assignments_split2 <- predict_pam(split2, pam2, s2)
assignments_split3 <- predict_pam(split3, pam3, s3)
relabel <- function(cluster_vec, map) {
map[cluster_vec]
}
assignments_split2_aligned <- relabel(assignments_split2, map12)
assignments_split3_aligned <- relabel(assignments_split3, map13)
split1$cluster_model1 <- assignments_split1
split2$cluster_model2 <- assignments_split2_aligned
split3$cluster_model3 <- assignments_split3_aligned
colnames(split1)[colnames(split1) == "cluster_model1"] <- "clusterLabel"
colnames(split2)[colnames(split2) == "cluster_model2"] <- "clusterLabel"
colnames(split3)[colnames(split3) == "cluster_model3"] <- "clusterLabel"
full_data_with_clusters <- rbind(split1, split2, split3)
cluster1_data <- subset(full_data_with_clusters, clusterLabel == 1)
cluster2_data <- subset(full_data_with_clusters, clusterLabel == 2)
cluster3_data <- subset(full_data_with_clusters, clusterLabel == 3)
cluster4_data <- subset(full_data_with_clusters, clusterLabel == 4)
cluster5_data <- subset(full_data_with_clusters, clusterLabel == 5)
c(mean(full_data_with_clusters$marital.status.Married.civ.spouse), mean(full_data_with_clusters$race.White), mean(full_data_with_clusters$incomeGreaterThan50k), mean(full_data_with_clusters$sex.Male))
cluster1_data #Married, White males potentially
c(mean(cluster1_data$incomeGreaterThan50k), mean(cluster1_data$race.White), mean(cluster1_data$marital.status.Married.civ.spouse), mean(cluster1_data$sex.Male))
cluster2_data #neverMarried, poor, males
c(mean(cluster2_data$marital.status.Never.married), 1 - mean(cluster2_data$incomeGreaterThan50k), mean(cluster2_data$race.White), mean(cluster2_data$sex.Male))
predict_pam <- function(newdata, pam_obj, df_train) {
df_train <- df_train[, -ncol(df_train)]
# Extract medoids from original training data
medoids <- df_train[pam_obj$medoids, , drop = FALSE]
# Build joint matrix for daisy()
both <- rbind(medoids, newdata)
# Distance matrix
dm <- daisy(both, metric = "gower") |> as.matrix()
k <- nrow(medoids)
dist_to_medoids <- dm[(k+1):(k + nrow(newdata)), 1:k]
# Assign nearest medoid
apply(dist_to_medoids, 1, which.min)
}
assignments_split1 <- predict_pam(split1, pam1, s1)
cluster2_data #neverMarried, poor, males
c(mean(cluster2_data$marital.status.Never.married), 1 - mean(cluster2_data$incomeGreaterThan50k), mean(cluster2_data$race.White), mean(cluster2_data$sex.Male))
cluster3_data #college dropouts/inschool, never married, women
#mean for full_data for some college is 0.221
c(1 - mean(cluster3_data$incomeGreaterThan50k), mean(cluster3_data$sex.Female), mean(cluster3_data$education.Some.college), mean(cluster3_data$marital.status.Never.married), mean(cluster3_data$race.White))
cluster4_data # divorced, women
#Average divorce rate in the data set is 0.138 compared to 0.72 here
c(mean(cluster4_data$marital.status.Divorced), 1 - mean(cluster4_data$incomeGreaterThan50k), mean(cluster4_data$sex.Female), mean(cluster4_data$race.White))
cluster5_data #highest education highschool, married, male,
#highest education highschool mean for full data is 0.326
c(mean(cluster5_data$race.White), mean(cluster5_data$sex.Male), mean(cluster5_data$marital.status.Married.civ.spouse))
muAge <- mean(rule_data$age)
sdA <- sd(rule_data$age)
mu1 <- mean(cluster1_data$age)*sdA + muAge
mu2 <- mean(cluster2_data$age)*sdA + muAge
mu3 <- mean(cluster3_data$age)*sdA + muAge
mu4 <- mean(cluster4_data$age)*sdA + muAge
mu5 <- mean(cluster5_data$age)*sdA + muAge
c(mu1, mu2, mu3, mu4, mu5)
rule_data <- rule_data %>%
mutate(marital.status = as.factor(marital.status),
marital.status = dplyr::recode(marital.status,
"Married-civ-spouse" = "Married",
"Married-spouse-absent" = "Spouse Absent"))
library(ggplot2)
ggplot(rule_data, aes(x = marital.status)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Marital Status Proportions",
x = "Marital Status",
y = "Proportion")
#ggsave("barplotM.png")
prop.table(table(rule_data$race))
rule_data$income <- gsub("\\.", "", rule_data$income)
income <- ggplot(rule_data, aes(x = income)) +
geom_bar(aes(y = after_stat(prop), group = 1), fill = "#0047BA") +
labs(title = "Income Proportions",
x = "Income",
y = "Proportion")
ggsave('income.png')
plot(density(rule_data$education.num),
main = "Density Plot of Education",
xlab = "Years of Education (starting from 4th grade)",
ylab = "Density",
xaxt = "n",             # suppress default x-axis
col = "#0047BA",
lwd = 2)
# Add custom x-axis
axis(side = 1, at = seq(0, 16, by = 2))
