---
title: "Final Project Workbook"
output:
  pdf_document: default
  html_document: default
date: "2025-11-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Mining Final Project

## Authors: James Christensen and Mohneesh Daksh

```{r}
full_data <- read.csv('data/data_mining.csv')
head(full_data)
```

### Data Preprocessing

```{r}
# 1. Remove unnecessary ID/index column
full_data$X <- NULL

# 2. Replace '?' with NA (for missing values)
full_data[full_data == "?"] <- NA

# 3. Check how many missing values per column
colSums(is.na(full_data))

# 4. Remove rows with missing values
full_data <- na.omit(full_data)
colSums(is.na(full_data))

# 4.1 Clean income variable column
full_data$income <- gsub("\\.", "", full_data$income)

# 5. Scale numeric columns (regularization)
library(dplyr)
full_data_scaled <- full_data %>%
mutate(across(where(is.numeric), scale))

# 6. Convert character columns to factors
full_data_scaled[sapply(full_data_scaled, is.character)] <- 
  lapply(full_data_scaled[sapply(full_data_scaled, is.character)], as.factor)

# 7. One-hot encode categorical variables (dummy variables)
library(caret)
dummy <- dummyVars(" ~ .", data = full_data_scaled)
full_data_scaled <- data.frame(predict(dummy, newdata = full_data_scaled))

#7.2 Drop unnecessary income column
full_data_scaled <- full_data_scaled[, - (ncol(full_data_scaled) - 1)]

#7.3 Rename the income column
colnames(full_data_scaled)[ncol(full_data_scaled)] <- "incomeGreaterThan50k"

# 8. Check structure of the processed dataset
str(full_data_scaled)

# 9. Preview cleaned and processed data
head(full_data_scaled)
```

The raw dataset contained 48,842 observations and 16 variables describing demographic and income information. During preprocessing, the index column (X) was removed, and missing values represented by “?” were converted to NA and omitted. All categorical variables such as workclass, education, and marital.status were converted into factors and later encoded into dummy variables using one-hot encoding, expanding the dataset to 112 attributes. Finally, all numeric features were standardized to ensure that variables operate on a comparable scale for subsequent analysis. After cleaning and transformation, the final dataset contained 46,443 valid records and 112 numeric predictors, ready for further data mining tasks.


### Data Description

The dataset used in this project was taken from the UCI Machine Learning Repository. It contains census data that was originally collected by the U.S. Census Bureau and published in 1996. The dataset includes information about adults such as their age, education, occupation, hours worked per week, and other demographic details.

The main goal of this dataset is to predict whether a person earns more than $50,000 a year based on their personal and work-related attributes. We found this dataset interesting because it involves real-world social and economic factors, and it allows us to explore patterns that might be linked to income levels. Even though it is an older dataset, it is still widely used today to practice data preprocessing, feature engineering, and machine learning classification tasks.


### Association algorithm

Here we do Market Basket Analysis on the dataset. This doesnt make sense to perform on many of the columns and we need to adjust the data set in order to do this. To find interesting trends we will assess whether there are any strong associations between *education, marital-status, occupation, race, sex, native-country*


```{r}
ruleData <- subset(full_data, select = c(education, marital.status, occupation, race, sex, native.country))

ruleData <- as.data.frame(model.matrix(~ . -1, data = ruleData))
head(ruleData)
```

Rule data is now a dataset where each column has been one hot encoded. As such, there are 84 columns in the data set and every variable level is indicated by a 1 in the appropriate column.

When it comes to itemsets and rule generation, it seems appropriate to have much lower support and confidence. We are dealing with census data with many, many levels. If we have a high support, we are going to miss rules especially from minority races or less common occupations. Since these are still interesting, it seems appropriate to keep a support of *1%*

The confidence is kept at 50% so that we can find all interesting rules. This was the recommended confidence level by chatGPT to ensure that patterns worth investigating are captured.
```{r}
library(arules)
data_transactions <- as(as.data.frame(lapply(ruleData, function(x) as.logical(x))), "transactions")

itemsets <- apriori(data_transactions, parameter = list(target = 'frequent itemsets', support = 0.01))
rules <- apriori(data_transactions, parameter = list(support = 0.01, confidence = 0.5))
```

```{r}
inspect(itemsets)
```
The algorithm was able to return 824 itemsets which is quite a few. The rules with high confidence are as follows:

```{r}
inspect(rules)
```

We have a lot of candidate rules, as such it seems appropriate to filter them by lift. It also seems appropriate to remove redundant rules as there seems to be alot of them. The 10 rules with the most lift are:

```{r}
rules <- rules[!is.redundant(rules)]
inspect(sort(rules, by = 'lift')[1:10])
```

The first 3 rules aren't very interesting. They basically say that if someone went to professional school then they have a professional occupation. What is remarkable is that they have a lift of 5

The next 4 rules all describe the same relationship. That being {some college, an other service, being white, from the US} -> {never married}. This is kinda interesting. No gender is associated with this one, but it may be interesting to investigate how this is associated with age. The rule has a lift of 1.95 which means that someone with some college, is white, from the US, and works in an other service is almost twice as likely to be unmarried. But again, this may be due to age, but that would have to be investigated.

The next rule is interesting as well. It is {Professional School, Male} -> {Married civily}. This rule has a lift of 1.73. So this means that if someone is male and has gone to professional school (such as to be a doctor or lawyer) he is 73% more likely to be married than normal.

The next rule is that {married, machine operator or machine inspector, from the US} -> {highest education high school}. This rule has a lift of 1.72. This means that individuals who are married, machine operators (or machine inspectors), and from the US are 72% more likely to only have a high school education.

#### Further rule analysis

There are 63 rules with a lift of more than 1.5. We will go through every one of these and document the interesting rules.

```{r}
inspect(sort(rules, by = 'lift')[11:20])
```

The next interesting rule is that {male, other service, from the US} -> {not married}. This rule has a lift of 1.68.

The other interesting rule from this sections is {High School Education, higher-level white-collar jobs (executives, managers, administrators), white, male, from the US} -> {Married civily}. Rule has a lift of 1.668

```{r}
inspect(sort(rules, by = 'lift')[21:30])
```

The only new interesting rule from this section is {Master's degree, male} -> {Married Civily}. This rule has a lift of 1.618.

```{r}
inspect(sort(rules, by = 'lift')[31:40])
```

There are no new interesting rules from this section, just some clarifications to the ones above.

```{r}
inspect(sort(rules, by = 'lift')[41:50])
```

The interesting rule from this section is {janitor/warehouse worker, male, from the US} -> {never married} has a lift of 1.558. 

```{r}
inspect(sort(rules, by = 'lift')[51:63])
```

One of the interesting rules that popped up had to do with the occupation of sales. I found it interesting, but some of the aspects seemed redundant, so I looked deeper into the rules. A distilled version of the rule is that {works in sales, male} -> {married civily} with a lift of 1.40. This means that a male who does sales is 40% more likely to be married than the public.

Perhaps the most stunning rule I've found is that {middle school education (7th/8th grade), male} -> {married civily} with a lift of 1.548. This means that males with only a middle school education are 55% more likely to be married than the general public.

```{r}
inspect(sort(rules, by = 'lift')[64:75])
```

This further analysis helped me confirm some actual associations and find some rather stunning new rules. One of which is that {male, Vocational Associate's degree} -> {Married Civily} with a lift of 1.456.

While this rule analysis has been interesting. I think it would be beneficial to do a more distilled analysis. One that includes occupation, race (not including white), sex, age, education, and marital status. I think this could uncover some interesting rules.

### Classification algorithm

library(caret)
library(randomForest)

# Setting seed to ensure results are reproducible
set.seed(123)  

# Cleaning the income variable by removing formatting inconsistencies

# Removing trailing periods
full_data$income <- gsub("\\.", "", full_data$income)

# Correcting inconsistent labels <50K to <=50K
full_data$income <- gsub("^<50K$", "<=50K", full_data$income)

# Converting income (target variable) to a factor
full_data$income <- as.factor(full_data$income)

# Verifying factor levels
levels(full_data$income)

# Just in case: drop any remaining NAs (should be none after preprocessing)
full_data <- na.omit(full_data)

# Creating a one-hot encoding model for all predictor variables (excluding the target 'income')
dummy_model <- dummyVars(income ~ ., data = full_data)

# Matrix/data frame of predictors (all numeric after encoding)
x_all <- predict(dummy_model, newdata = full_data)
x_all <- as.data.frame(x_all)

# Extracting the target variable into a separate vector
y_all <- full_data$income

# Splitting into training (70%) and test (30%) sets, stratified by income
train_index <- createDataPartition(y_all, p = 0.7, list = FALSE)

x_train <- x_all[train_index, ]
x_test  <- x_all[-train_index, ]

y_train <- y_all[train_index]
y_test  <- y_all[-train_index]

# Check dataset dimensions and class distribution to verify successful splitting
dim(x_train)
dim(x_test)
table(y_train)
table(y_test)


# Training a Random Forest classifier on the training set

rf_model <- randomForest(
  x = x_train,
  y = y_train,
  ntree = 500,                            # number of trees in the forest
  mtry = floor(sqrt(ncol(x_train))),     # number of predictors sampled per split
  importance = TRUE                     # compute variable importance
)

rf_model


# Generating predictions on the test dataset

# Predict income classification labels for the test dataset
rf_pred_class <- predict(rf_model, newdata = x_test, type = "class")

# Predict class probabilities (needed for evaluation metrics like ROC curve)
rf_pred_prob <- predict(rf_model, newdata = x_test, type = "prob")

# Display the first few predicted class labels
cat("\nFirst few predicted classes:\n")
print(head(rf_pred_class))

# Display a confusion table comparing predicted vs actual values
cat("\nConfusion table (raw counts):\n")
print(table(Predicted = rf_pred_class, Actual = y_test))

head(rf_pred_class)
head(rf_pred_prob)

```{r}
library(randomForest)

```

```{r}
full_data_scaled[, ncol(full_data_scaled)] <- 
  as.factor(full_data_scaled[, ncol(full_data_scaled)])
train_index <- createDataPartition(
  y = full_data_scaled[, ncol(full_data_scaled)],
  p = 0.8,
  list = FALSE
)

train_data <- full_data_scaled[train_index, ]
test_data  <- full_data_scaled[-train_index, ]
```


```{r}
rf_model <- randomForest(
  x = train_data[, -ncol(train_data)],   # predictors
  y = train_data[,  ncol(train_data)],   # target
  ntree = 500,
  mtry = floor(sqrt(ncol(train_data) - 1)),
  importance = TRUE
)
```

```{r}
rf_preds <- predict(rf_model, test_data[, -ncol(test_data)])

```

```{r}
confusionMatrix(
  data = rf_preds,
  reference = test_data[, ncol(test_data)]
)
```

```{r}
rf_probs <- predict(
  rf_model, 
  test_data[, -ncol(test_data)], 
  type = "prob"
)[, 2] 

roc_obj <- roc(
  response = test_data[, ncol(test_data)],
  predictor = rf_probs
)

auc(roc_obj)

plot(roc_obj)
```


```{r}
importance(rf_model)
```

```{r}
varImpPlot(rf_model)
```


### Clustering algorithm

```{r}
library(cluster)
library(factoextra)
library(dplyr)
```


Findings the best number of clusters
```{r}
sampsize <- 2000
samp_idx <- sample(nrow(full_data_scaled), sampsize)
adult_sample <- full_data_scaled[samp_idx, ]

# 2. Gower on SAMPLE ONLY (this is safe)
gower_sample <- daisy(adult_sample, metric = "gower")

# 3. Find best k using PAM + silhouette
k_values <- 2:8
sil_width <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  pam_fit <- pam(gower_sample, k = k_values[i], diss = TRUE)
  sil_width[i] <- mean(silhouette(pam_fit)[, 3])
}

plot(k_values, sil_width, type = "b")

best_k <- 4

```


```{r}
set.seed(123)

n <- nrow(full_data_scaled)
split_id <- sample(rep(1:3, length.out = n))

split1 <- full_data_scaled[split_id == 1, ]
split2 <- full_data_scaled[split_id == 2, ]
split3 <- full_data_scaled[split_id == 3, ]

subsample_size <- 2000

s1 <- split1[sample(nrow(split1), subsample_size), ]
s2 <- split2[sample(nrow(split2), subsample_size), ]
s3 <- split3[sample(nrow(split3), subsample_size), ]

pam_run <- function(df, k) {
  gower_dist <- daisy(df, metric = "gower")
  pam(gower_dist, k = k, diss = TRUE)
}

k <- 4   # choose once and keep fixed

pam1 <- pam_run(s1, k)
pam2 <- pam_run(s2, k)
pam3 <- pam_run(s3, k)

s1$cluster <- pam1$clustering
s2$cluster <- pam2$clustering
s3$cluster <- pam3$clustering
```

```{r}
clusterData <- full_data_scaled
clusterData$clusterLabel <-  factor(apply(dist_to_medoids, 1, which.min))
```

Analyze clusters

```{r}

```


### Visualization

We can show different visualizations. It would probably be easiest to do one for the clustering algorithm or classification algorithm, but any of them would work

### Performance Evaluator

I think we could do an indepth discussion of the classification algorithm's performance. We could use a ROC curve and a confusion matrix.

# Creating a confusion matrix treating ">50K" as the positive class
cm <- confusionMatrix(
  rf_pred_class,
  y_test,
  positive = ">50K"
)

# Printing the confusion matrix and associated statistics
print(cm)

# A confusion matrix is generated using the test dataset to evaluate classification performance. The model achieved strong overall accuracy, correctly identifying most individuals with income <=50K. However, predicting the minority class (>50K) remains more challenging due to class imbalance. Precision and recall for the positive class were examined to understand how well the model detects high-income individuals. The F1-score provides a balanced assessment of these metrics and reflects the expected performance range for this dataset.